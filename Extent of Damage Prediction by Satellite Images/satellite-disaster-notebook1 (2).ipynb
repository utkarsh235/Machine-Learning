{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n'''for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport time\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras \nfrom keras.models import Sequential, Model\nfrom keras.applications import VGG16, VGG19, Xception, ResNet50, ResNet101, InceptionV3, InceptionResNetV2, MobileNetV2, DenseNet121, DenseNet169\nfrom keras.layers import Input, Flatten, Dense, Dropout\nfrom keras.utils import to_categorical","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"main_folder = '../input/xview2-damage-assessment'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = train_folder = main_folder + '/train/images'\nholdout_folder = main_folder + '/hold'\ntier3_folder = main_folder + '/tier3'\ntier3_images = tier3_folder + '/images'\nholdout_images = holdout_folder+'/images'\n\ntrain_labels_path = main_folder + '/train/labels'\nhold_labels_path = main_folder + '/hold/labels'\ntier3_labels_path = main_folder + '/tier3/labels'\n#train_labels_path = '../input/xview2-damage-assessment/train/labels'\n#hold_labels_path = '../input/xview2-damage-assessment/hold/labels'\n#tier3_labels_path = '../input/xview2-damage-assessment/tier3/labels'\n\n#train_images = train_folder = '../input/xview2-damage-assessment/train/images'\n#holdout_folder = '../input/xview2-damage-assessment/hold'\n#tier3_folder = '../input/xview2-damage-assessment/tier3'\n#tier3_images = tier3_folder + '/images'\nprint(os.listdir(holdout_folder))\nprint(os.listdir(tier3_folder))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info = pd.DataFrame(\n    {'Name': '_',\n     'index': '0',\n     'pre_path': '',\n     'post_path': '',\n     'paths': 'paths'\n    },index=range(len(os.listdir(train_folder)) + len(os.listdir(holdout_folder+'/images'))+ len(os.listdir(tier3_images)))\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(os.listdir(train_images)))\nprint(len(os.listdir(holdout_images)))\nprint(len(os.listdir(tier3_images)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\nfor i in range(len(os.listdir(train_folder))+ len(os.listdir(holdout_images))+ len(os.listdir(tier3_images))):\n    if i % 1000 == 0:\n        print(i)\n    if i < len(os.listdir(train_folder)):\n        path = train_folder + '/' + os.listdir(train_folder)[i]\n        broken = os.listdir(train_folder)[i].split('_')\n\n        pre_path = train_folder + '/' + broken[0] + '_' + broken[1] + '_' + 'pre_disaster.png'  \n        post_path = train_folder + '/' + broken[0] + '_' + broken[1] + '_' + 'post_disaster.png'\n    elif i > len(os.listdir(train_folder)) and i < (len(info) - len(os.listdir(tier3_images))):\n        path = holdout_images + '/' + os.listdir(holdout_images)[i-len(os.listdir(train_folder))]\n        broken = os.listdir(holdout_images)[i-len(os.listdir(train_folder))].split('_')\n\n        pre_path = holdout_images + '/' + broken[0] + '_' + broken[1] + '_' + 'pre_disaster.png'  \n        post_path = holdout_images + '/' + broken[0] + '_' + broken[1] + '_' + 'post_disaster.png'\n    else:\n        path = tier3_images + '/' + os.listdir(tier3_images)[i-len(os.listdir(train_folder))-len(os.listdir(holdout_images))]\n        broken = os.listdir(tier3_images)[i-len(os.listdir(train_folder))-len(os.listdir(holdout_images))].split('_')\n\n        pre_path = tier3_images + '/' + broken[0] + '_' + broken[1] + '_' + 'pre_disaster.png'  \n        post_path = tier3_images + '/' + broken[0] + '_' + broken[1] + '_' + 'post_disaster.png'\n        \n        \n    info['Name'][i] = broken[0] + '_' + broken[1]\n    info['index'][i] = broken[1]\n    info['pre_path'][i] = pre_path\n    info['post_path'][i] = post_path\n    info['paths'][i] = [pre_path, post_path]\nend = time.time()\nprint(f'Operation executed in {end-start} seconds')","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info.drop_duplicates(subset='pre_path', keep='first', inplace=True)\ninfo.reset_index(inplace=True)\ninfo.drop(['level_0'], axis=1, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(info['pre_path'][0])\nprint(info['post_path'][0])\nprint(info['paths'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start = time.time()\ninfo['labels'] = ''\ninfo['disaster_type'] = ''\nfor i in range(len(info)):\n    if i%500 == 0:\n        print(i)\n    \n    if i < len(os.listdir(train_folder))//2:\n        pass\n        json_file = train_labels_path + '/' + info['Name'][i] + '_' + 'post_disaster.json'\n        jso_post = pd.read_json(json_file)\n        info['disaster_type'][i] =  jso_post['metadata']['disaster_type']\n        length = len(jso_post.iloc[1,:][0])\n        #print(jso_post.iloc[1, :][0][0]['properties']['subtype'])\n        for j in range(length):\n            info['labels'][i] = info['labels'][i] + jso_post.iloc[1,:][0][j]['properties']['subtype'] + '_'\n    \n    elif i > len(os.listdir(train_folder))//2 and i <= ( len(os.listdir(train_folder)) + len(os.listdir(holdout_images)) )//2:\n        json_file = hold_labels_path + '/' + info['Name'][i] + '_' + 'post_disaster.json'\n        #print(json_file)\n        jso_post = pd.read_json(json_file)\n        info['disaster_type'][i] =  jso_post['metadata']['disaster_type']\n        length = len(jso_post.iloc[1,:][0])\n        #print(jso_post.iloc[1, :][0][0]['properties']['subtype'])\n        for j in range(length):\n            info['labels'][i] = info['labels'][i] + jso_post.iloc[1,:][0][j]['properties']['subtype'] + '_'\n    \n    else:\n        json_file = tier3_labels_path + '/' + info['Name'][i] + '_' + 'post_disaster.json'\n        jso_post = pd.read_json(json_file)\n        info['disaster_type'][i] =  jso_post['metadata']['disaster_type']\n        length = len(jso_post.iloc[1,:][0])\n        #print(jso_post.iloc[1, :][0][0]['properties']['subtype'])\n        for j in range(length):\n            info['labels'][i] = info['labels'][i] + jso_post.iloc[1,:][0][j]['properties']['subtype'] + '_'\n        \nend = time.time()\nprint(end-start)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['simplified_labels'] = 0\nfor i in range(len(info)):\n    info['simplified_labels'][i] = info['labels'][i].split('_')\n    info['simplified_labels'][i] = info['simplified_labels'][i][:-1]\n    \nfor i in range(len(info)):\n    if info['simplified_labels'][i] == []:\n        info['simplified_labels'][i] = ['no-damage']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['simplified_labels'].dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor j in range(len(info)):\n    if info['simplified_labels'][j] == list(['un-classified']):\n        count+=1\ncount","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count = 0\nfor j in range(len(info)):\n    if info['simplified_labels'][j] == list([\"no-damage\"]):\n        count += 1\nprint(count)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(10):\n    pre = info['paths'][i][0]\n    post = info['paths'][i][1]\n    pre = cv2.imread(pre)\n    post = cv2.imread(post)\n    print(pre.shape, post.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Every pre and post image is of shape (1024 x 1024 x 3)","metadata":{}},{"cell_type":"code","source":"len(np.where(info['labels'] == '')[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['disaster_type'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['regression_labels'] = 1.33\n\ndef counter(lst):\n    a = Counter(lst)\n    num_no_damage = a['no-damage']\n    num_minor_damage = a['minor-damage']\n    num_major_damage = a['major-damage']\n    num_destroyed = a['destroyed']\n    num_unclassified = a['un-classified']\n    return num_no_damage, num_minor_damage, num_major_damage, num_destroyed, num_unclassified\n\nfor i in range(len(info)):\n    num_no_damage, num_minor_damage, num_major_damage, num_destroyed, num_unclassified = counter(info['simplified_labels'][i])\n    reg_value = ( (num_no_damage*0)+(num_minor_damage*0.33)+(num_major_damage*0.66)+(num_destroyed*1)+(num_unclassified*0.50) ) / len(info['simplified_labels'][i]) \n    info['regression_labels'][i] = reg_value       \n            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ninfo['regression_labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info['regression_labels_str'] = str(info['regression_labels'])\nfor i in range(len(info)):\n    info['regression_labels_str'][i] = str(info['regression_labels'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(info)):\n    info['regression_labels'][i] = float(info['regression_labels_str'][i][0:4])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_copied = info.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_sampled = info_copied.sample(10).reset_index()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_sampled","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = info_copied.drop(['regression_labels'], axis=1)\nY = info_copied['regression_labels']\ntrain_x, val_x, train_y, val_y = train_test_split(X, Y, shuffle=True, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size=40\n#sampled_info = info.sample(batch_size).reset_index()\ndef train_generator(sampled_info, batch_size=batch_size):\n    while True:\n        batch_train_images = []\n        batch_train_labels = []\n        sampled_dataset = sampled_info.sample(batch_size).reset_index()\n        for i in range(len(sampled_dataset)):\n            pre_img_path = sampled_dataset['pre_path'][i]\n            post_img_path = sampled_dataset['post_path'][i]\n            preprocessed_array = read_and_process_img(pre_img_path, post_img_path)\n            batch_train_images.append(preprocessed_array)\n            batch_train_labels.append(sampled_dataset['regression_labels'][i])\n            #print(i)\n        \n        batch_train_images = np.array(batch_train_images)\n        batch_train_labels = np.array(batch_train_labels)\n        yield(batch_train_images, batch_train_labels)\n\ndef read_and_process_img(pre_img_path, post_img_path):\n    img_size=224\n    pre_img = cv2.imread(pre_img_path)\n    post_img = cv2.imread(post_img_path)\n    pre_img = cv2.resize(pre_img, (img_size, img_size))\n    post_img = cv2.resize(post_img, (img_size, img_size))\n    final_array = np.concatenate((pre_img, post_img), axis=2)\n    final_array = final_array / 255\n    return final_array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x = pd.concat([train_x, train_y], axis=1)\nval_x = pd.concat([val_x, val_y], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#trained = train_generator(info)\nfor_train = train_generator(train_x)\nfor_val = train_generator(val_x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"baseModel = tf.keras.applications.Xception(\n    include_top=False,\n    weights = None,\n    #weights=\"imagenet\",\n    input_tensor=Input(shape=(224, 224, 6)),\n    input_shape=None,\n    pooling=None,\n    classes=1\n)\n#baseModel = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\nheadModel = baseModel.output\nheadModel = Flatten(name=\"flatten\")(headModel)\nheadModel = Dense(20, activation=\"sigmoid\")(headModel)\nheadModel = Dropout(0.5)(headModel)\nheadModel = Dense(1, activation=\"sigmoid\")(headModel) # Try using sigmoid as well\n\nfor layer in baseModel.layers:\n    layer.trainable = False\n    \nmodel = Model(inputs=baseModel.input, outputs=headModel)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(\n    optimizer = 'rmsprop',\n    loss = 'mean_squared_error',\n    metrics = ['accuracy', 'mse']\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(for_train, steps_per_epoch=len(train_x)//batch_size, epochs=5, validation_data=for_val, validation_steps=len(val_x)//batch_size)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output_result_path = './'\nhist_df = pd.DataFrame(history.history) \n\n# or save to csv: \nhist_csv_file = output_result_path + 'history.csv'\nwith open(hist_csv_file, mode='w') as f:\n    hist_df.to_csv(f)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Insert Output Path Below\noutput_path = './'\ntf.keras.models.save_model(model, output_path+'model_xception.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_size = 224\nsome_images = []\nsome_labels = []\nfor i in range(200):\n    pre_img = info['pre_path'][i]\n    post_img = info['post_path'][i]\n    pre = cv2.imread(pre_img)\n    post = cv2.imread(post_img)\n    pre = cv2.resize(pre, (img_size,img_size))\n    post = cv2.resize(post, (img_size,img_size))\n    final = np.concatenate((post, pre), axis=2)\n    final = final/255\n    some_images.append(final)\n    some_labels.append(info['regression_labels'][i])\nsome_images = np.array(some_images)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(some_images)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"some_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"info_copied['regression_labels'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(some_images, np.array(some_labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}